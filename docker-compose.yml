services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: ollamapilot-postgres
    environment:
      POSTGRES_DB: ${DB_NAME:-ollamapilot}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - chat_ollama_network
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ollamapilot-api
    ports:
      - "8080:8080"
    environment:
      - DB_TYPE=${DB_TYPE:-postgres}
      - DB_HOST=${DB_HOST:-postgres}
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-ollamapilot}
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_SSL_MODE=${DB_SSL_MODE:-disable}
      - OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434}
      - PORT=${PORT:-8080}
      - ENV=${ENV:-production}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      - MAX_CONCURRENT_CHATS=${MAX_CONCURRENT_CHATS:-100}
      - READ_TIMEOUT=${READ_TIMEOUT:-30s}
      - WRITE_TIMEOUT=${WRITE_TIMEOUT:-30s}
      - ENABLE_SEMANTIC_MEMORY=${ENABLE_SEMANTIC_MEMORY:-true}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
      - MAX_CONTEXT_RESULTS=${MAX_CONTEXT_RESULTS:-5}
    volumes:
      - ./web:/app/web:ro
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - chat_ollama_network
    security_opt:
      - no-new-privileges:true

  ollama:
    image: ollama/ollama:latest
    container_name: ollamapilot-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_HOST=${OLLAMA_INTERNAL_HOST:-0.0.0.0}
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-2}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - chat_ollama_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  postgres_data:
    driver: local
  ollama_models:
    driver: local

networks:
  chat_ollama_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16